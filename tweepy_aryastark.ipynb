{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "import pymongo\n",
    "from sqlalchemy import func\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The consumer keys can be found on your application's Details\n",
    "# page located at https://dev.twitter.com/apps (under \"OAuth settings\")\n",
    "access_token = \"114010579-jAnTPiNwF9krPFR68yX61Qqtw2NluLs8dNQaoi7E\"\n",
    "access_token_secret = \"eude9xgmDMSvLIaylw2tNJTd0LXd2i7wfKZFA9zWKNqIp\"\n",
    "consumer_key = \"j2BhP2Q3LFCzpPQ5o0Xx4GuY7\"\n",
    "consumer_secret = \"vkcmHL1U71cpqdn1LBl7GY4cdJIkvFotKAyZeChIuHBoq5aQ2j\"\n",
    "\n",
    "master_list = ([\"#NotToday\", \n",
    "              \"#TheLongNight.\", \n",
    "              \"#GameofThrones\",  \n",
    "              \"#Dracarys\",  \n",
    "              \"#GOT\", \n",
    "              \"#GOTS8\", \n",
    "              \"#ForTheThrone\", \n",
    "              \"#DaenerysTargaryen\", \n",
    "              \"#JonSnow\", \n",
    "              \"#NightKing\", \n",
    "              \"#CerseiLannister\", \n",
    "              \"#AryaStark\", \n",
    "              \"#JaimeLannister\", \n",
    "              \"#TyrionLannister\", \n",
    "              \"#SansaStark\",\n",
    "              \"#BranStark\", \n",
    "              \"#BrienneOfTarth\", \n",
    "              \"#DavosSeaworth\", \n",
    "              \"#EuronGreyjoy\", \n",
    "              \"#JorahMormont\", \n",
    "              \"#Greyworm\", \n",
    "              \"#Melisandre\", \n",
    "              \"#Missandei\", \n",
    "              \"#SamwellTarly\", \n",
    "              \"#TheonGreyjoy\", \n",
    "              \"#Varys\", \n",
    "              \"#TheHound\"\n",
    "             ])\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-22-fbc943473f53>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-fbc943473f53>\"\u001b[1;36m, line \u001b[1;32m62\u001b[0m\n\u001b[1;33m    end_run = time.time()\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "# Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "# Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "        # retweeted_status.full_text - full text of the tweet\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "# Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "for tweet in tweet_list:\n",
    "# Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "#             except AttributeError:  # Not a Retweet\n",
    "#                 text = tweet.full_text\n",
    "# Add the 11 variables to the empty list - ith_tweet:\n",
    "                ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "# Append to dataframe - db_tweets\n",
    "                db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "# increase counter - noTweets  \n",
    "                noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920) #15 minute sleep time\n",
    "# Once all runs have completed, save them to a single csv file:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "# Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_sahkprotests_tweets.csv'\n",
    "# Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = '#DaenerysTargaryen OR \"Daenerys Targaryen\"'\n",
    "char_name = \"Daenerys Targaryen\"\n",
    "\n",
    "dates = [[\"2019-05-19\", \"2019-05-20\"], \n",
    "         [\"2019-05-20\", \"2019-05-21\"] \n",
    "        ]\n",
    "\n",
    "date_since = \"\"\n",
    "date_until = \"\"\n",
    "tweet_text = []\n",
    "tweet_timestamp = []\n",
    "tweet_date = []\n",
    "tweet_char = []\n",
    "tweet_sen = []\n",
    "tweet_pol = []\n",
    "tweet_sub = []\n",
    "\n",
    "for date_list in dates:\n",
    "    date_since = date_list[0]\n",
    "    date_until = date_list[1]\n",
    "    tweets = tw.Cursor(api.search,\n",
    "            q=search_words,\n",
    "            lang=\"en\",\n",
    "            since=date_since, until=date_until, encode=\"utf-8\").items(500)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet_date.append(tweet.created_at.strftime('%m/%d/%Y'))\n",
    "        tweet_timestamp.append(tweet.created_at.strftime('%m/%d/%Y %H:%M:%S'))\n",
    "    \n",
    "        analysis = TextBlob(tweet.text)\n",
    "\n",
    "        senti = \"\"\n",
    "        polarity = 0.0\n",
    "        subjectivity = 0.0\n",
    "    \n",
    "        if analysis.sentiment[0]>0:\n",
    "           senti = \"Pos\"\n",
    "        elif analysis.sentiment[0]<0:\n",
    "           senti = \"Neg\"\n",
    "        else:\n",
    "           senti = \"Neu\"\n",
    "    \n",
    "        polarity = float(analysis.sentiment[0])\n",
    "        subjectivity = float(analysis.sentiment[1])\n",
    "\n",
    "        tweet_text.append(tweet.text)\n",
    "        tweet_char.append(char_name)\n",
    "        tweet_pol.append(polarity)\n",
    "        tweet_sen.append(senti)\n",
    "        tweet_sub.append(subjectivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "  \"text\": tweet_text,\n",
    "  \"date\" : tweet_date,\n",
    "  \"char\" : tweet_char,\n",
    "  \"sentiment\" : tweet_sen,\n",
    "  \"polarity\" : tweet_pol,\n",
    "  \"subjectivity\" : tweet_sub,\n",
    "  \"timestamp\" : tweet_timestamp\n",
    "})\n",
    "\n",
    "df = df.dropna()\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Remove all the special characters\n",
    "    test_preproc = re.sub(r'\\W', ' ', str(row[\"text\"]))\n",
    "\n",
    "    # remove all single characters\n",
    "    test_preproc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', test_preproc)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    test_preproc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', test_preproc) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    test_preproc = re.sub(r'\\s+', ' ', test_preproc, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    test_preproc = re.sub(r'^b\\s+', '', test_preproc)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    test_preproc = test_preproc.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    test_preproc = test_preproc.split()\n",
    "\n",
    "    test_preproc = [stemmer.lemmatize(word) for word in test_preproc]\n",
    "    comb_string = ' '.join(test_preproc)\n",
    "    df.loc[index, \"Preprocess\"] = comb_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DaenerysTargaryen_19_21.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
